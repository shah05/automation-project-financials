{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A : Data Massage\n",
    "\n",
    "### 1. Declare input file names.\n",
    "\n",
    "File needs to be in CSV format for faster loading.\n",
    "\n",
    "#### Input 1 = ME2L_W01.CSV\n",
    "#### Input 2 = ME2L_W02.CSV\n",
    "\n",
    "Place the 2 input files in the current directory as the script file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# start = time.time()\n",
    "# me2l_w01_file = \"test/me2l_w01x.xlsx\"\n",
    "# df1 = pd.read_excel(me2l_w01_file,sheet_name='Raw');\n",
    "# end = time.time()\n",
    "# print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "me2l_w01_file = \"test/me2l_w01.csv\"\n",
    "me2l_w02_file = \"test/me2l_w02.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Read input files\n",
    "\n",
    "If file contains large data sets with alot of columns, reading will take a bit of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TM36250\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3146: DtypeWarning: Columns (5,9,12,14) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "# Load first input file\n",
    "df1 = pd.read_csv(me2l_w01_file,parse_dates=True, encoding='ISO-8859-1');\n",
    "# df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load second input file\n",
    "df2 = pd.read_csv(me2l_w02_file,parse_dates=True);\n",
    "# df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Extract PO with correct CAPEX Category from ME2L_W02 \n",
    "\n",
    "Capex Category = (A, N, P, X).\n",
    "But how to check if the project is deployment,operation related ? Any keyword or vendor as filters ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "capex_w02 = df2[(df2['Acct Assignment Cat.'] == 'A') |\n",
    "                (df2['Acct Assignment Cat.'] == 'N') |\n",
    "                (df2['Acct Assignment Cat.'] == 'P') |\n",
    "                (df2['Acct Assignment Cat.'] == 'X')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Append extraction from ME2L_W02 to ME2L_W01\n",
    "\n",
    "Data will be pasted at the bottom of ME2L_W01 data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.append(capex_w02);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Remove unwanted columns.\n",
    "\n",
    "This will allow processing of data faster since unwanted data is discarded.Columns removed:\n",
    "- Deletion Indicator\n",
    "- Req. Tracking Number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['Deletion Indicator' 'Req. Tracking Number'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-d1458e38eabf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Deletion Indicator'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Req. Tracking Number'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4161\u001b[0m                 \u001b[0mweight\u001b[0m  \u001b[1;36m1.0\u001b[0m     \u001b[1;36m0.8\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4162\u001b[0m         \"\"\"\n\u001b[1;32m-> 4163\u001b[1;33m         return super().drop(\n\u001b[0m\u001b[0;32m   4164\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4165\u001b[0m             \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   3885\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3886\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3887\u001b[1;33m                 \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3888\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3889\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[1;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[0;32m   3919\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3920\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3921\u001b[1;33m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3922\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3923\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   5280\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5281\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5282\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{labels[mask]} not found in axis\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5283\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5284\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['Deletion Indicator' 'Req. Tracking Number'] not found in axis\""
     ]
    }
   ],
   "source": [
    "df1 = df1.drop(columns=['Deletion Indicator','Req. Tracking Number']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Insert 'Vendor Code' and 'Vendor' column at the begining.\n",
    "\n",
    "Extract Vendor Code info and Vendor Name info from 'Vendor/supplying plant' column.\n",
    "\n",
    "Once this process done, remove the 'Vendor/supplying plant' column since it is redundant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = df1[\"Vendor/supplying plant\"].str.split(\" \", n = 1, expand = True) \n",
    "# df1.insert(0,'Vendor Code', new[0]);\n",
    "# df1.insert(1,'Vendor', new[1]);\n",
    "\n",
    "# # df_vendor = new[0] + new[1]\n",
    "\n",
    "# new[0][1]\n",
    "# new[0][1]\n",
    "new3={'Vendor Code': new[0],'Vendor Correct': new[1]}\n",
    "\n",
    "df_new = pd.DataFrame(new3, columns = ['Vendor Code', 'Vendor Correct'])\n",
    "\n",
    "df_new.drop_duplicates(inplace=True)\n",
    "df_new.drop_duplicates(subset='Vendor Code',keep='first',inplace=True)\n",
    "df_new.sort_values(by=['Vendor Code'],inplace=True)\n",
    "df_new.head(100)\n",
    "\n",
    "df_new.to_csv('test/vlookup.csv');\n",
    "\n",
    "# df= df_new.drop_duplicates()\n",
    "\n",
    "# df = df.drop_duplicates(subset='Vendor Code',keep='first')\n",
    "# df.sort_values(by=['Vendor Correct'])\n",
    "\n",
    "# print(df.count())\n",
    "\n",
    "\n",
    "# inner_join = pd.merge(df1,  \n",
    "#                       df,  \n",
    "#                       on ='Vendor Code',  \n",
    "#                       how ='inner') \n",
    "\n",
    "# inner_join.count()\n",
    "\n",
    "# df = inner_join.drop(columns=['Vendor']);\n",
    "\n",
    "# df.rename(columns={'Vendor Correct': 'Vendor'}, inplace=True)\n",
    "\n",
    "# df.to_csv('test/vlookup.csv');\n",
    "\n",
    "# df_new.count()\n",
    "\n",
    "# df_vendor.columns =['Vendor Code', 'Vendor Correct'] \n",
    "\n",
    "\n",
    "# df_vendor.head()\n",
    "\n",
    "# df1.head()\n",
    "# df1.insert(0,'Vendor Code', new[0]);\n",
    "# df1.insert(1,'Vendor', new[1]);\n",
    "\n",
    "# for index, row in df1.iterrows():\n",
    "#     print(index, row['Vendor'])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Honda Civic\n",
      "1 Toyota Corolla\n",
      "2 Ford Focus\n",
      "3 Audi A4\n"
     ]
    }
   ],
   "source": [
    "cars = {'Brand': ['Honda Civic','Toyota Corolla','Ford Focus','Audi A4'],\n",
    "        'Price': [22000,25000,27000,35000]\n",
    "        }\n",
    "cars2 = {'Brand': ['Honda Civic','Toyota Corolla','Ford Focus','Audi A4'],\n",
    "        'Speed': [112,180,250,350]\n",
    "        }\n",
    "\n",
    "\n",
    "df_cars2 = pd.DataFrame(cars2, columns = ['Brand', 'Speed'])\n",
    "\n",
    "df_cars = pd.DataFrame(cars, columns = ['Brand', 'Price'])\n",
    "\n",
    "for index, row in df_cars.iterrows():\n",
    "    print(index, row['Brand'])\n",
    "# new_df = pd.merge(df_cars,df_cars2,on ='Brand',how='inner')\n",
    "# new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Classify each PO according to CAPEX/OPEX category.\n",
    "\n",
    "Use values from 'Acc Assignment Cat.' as reference for classification.\n",
    "- CAPEX (A, N, P, X)\n",
    "- OPEX (F, K, Blank)\n",
    "\n",
    "Add a new column called 'Capex/Opex' at the end of the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add Capex/Opex Category\n",
    "df1.loc[ (df1['Acct Assignment Cat.'] == 'A') |\n",
    "        (df1['Acct Assignment Cat.'] == 'N') |\n",
    "        (df1['Acct Assignment Cat.'] == 'P') |\n",
    "        (df1['Acct Assignment Cat.'] == 'X'),'Capex/Opex'] = 'CAPEX'\n",
    "\n",
    "df1.loc[ (df1['Acct Assignment Cat.'] == 'F') |\n",
    "        (df1['Acct Assignment Cat.'] == 'K') |\n",
    "        (df1['Acct Assignment Cat.'] == ''),'Capex/Opex'] = 'OPEX'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Calculate Aging, PO Category & PO Year\n",
    "\n",
    "Add 2 new columns to input the Aging info:\n",
    "- 'Aging' = shows aging in number of days\n",
    "- 'Aging (Months & Days)' = shows aging in number of months and remaining days\n",
    "\n",
    "Check PO Category if the value in 'Document Date' column is similar to current year. If not then PO < Current Year\n",
    "\n",
    "Check PO Year using the 'Document Date' column. Extract year info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "def calculate_aging_days(doc_date,date_now):\n",
    "    po_date = datetime.datetime.strptime(doc_date, '%d/%m/%Y')\n",
    "    delta = now - po_date\n",
    "    return int(delta.days)\n",
    "\n",
    "def calculate_aging_months_days(aging_days):\n",
    "    months = int(aging_days/30)\n",
    "    remaining_days = int(aging_days%30)\n",
    "    return str(months) + ' months ' + str(remaining_days) + ' days'\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "df1['Aging Days'] = df1.apply(lambda row: calculate_aging_days(row['Document Date'],now),axis=1)\n",
    "df1['PO Year'] =  datetime.datetime.now().year\n",
    "df1['Aging (Months & Days)'] = df1.apply(lambda row: calculate_aging_months_days(row['Aging Days']),axis=1)\n",
    "\n",
    "df1['Aging Days'].head()\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Calculate Aging Category\n",
    "\n",
    "Classify each PO into the following categories:\n",
    "- ( <6 Months )\n",
    "- ( >6 Months )\n",
    "- ( >18 Months )\n",
    "\n",
    "Add a new column 'Aging Category' at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df1.loc[ (df1['Aging Days'] <= 182),'Aging Category'] = '<6 Months'\n",
    "df1.loc[ (df1['Aging Days'] > 182) & (df1['Aging Days'] < 540),'Aging Category'] = '>6 Months'\n",
    "df1.loc[ (df1['Aging Days'] > 540),'Aging Category'] = '>18 Months'\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Assign GR status\n",
    "\n",
    "Status can either be 'Open' or 'Closed' depending on there are still value to be delivered. Use the column 'Still to be delivered (value)' as reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df1['Still to be delivered (value)'] = df1['Still to be delivered (value)'].str.replace(',','')\n",
    "df1['Still to be delivered (value)'] = df1['Still to be delivered (value)'].astype(float)\n",
    "df1.loc[ (df1['Still to be delivered (value)'] > 0),'GR Status'] = 'Open'\n",
    "df1.loc[ (df1['Still to be delivered (value)'] == 0),'GR Status'] = 'Closed'\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Assign GRIR Status\n",
    "Status can be either 'Open' or 'Closed' depending on value in column 'Still to be invoiced (val.)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['Still to be invoiced (val.)'] = df1['Still to be invoiced (val.)'].str.replace(',','')\n",
    "df1['Still to be invoiced (val.)'] = df1['Still to be invoiced (val.)'].astype(float)\n",
    "df1['GRIR Status'] = df1['Still to be invoiced (val.)'].apply(lambda x: 'Open' if x > -0.1 and x < 0.1 else 'Closed')\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Assign PO Category\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1.loc[ (df1['GR Status'] == 'Closed') & (df1['GRIR Status'] == 'Closed'),'PO Status'] = 'Closed'\n",
    "# df1.loc[ (df1['GR Status'] == 'Open') | (df1['GRIR Status'] == 'Open'),'PO Status'] = 'Open'\n",
    "\n",
    "df1['PO Status'] = np.where((df1['GR Status'] == 'Closed') & (df1['GRIR Status'] == 'Closed'), 'Closed', 'Open')\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. Calculate Still to be delivered (MYR-Value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set USD Currency rate here.\n",
    "usd_currency_rate = 4.1\n",
    "\n",
    "df1['Still to be delivered (MYR-Value)'] = np.where(df1['Currency']== 'USD', df1['Still to be delivered (value)']*usd_currency_rate, df1['Still to be delivered (value)'])\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B : Prepare Report\n",
    "\n",
    "### 1. Write data to excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary column and create new dataframe\n",
    "df_final = df1.drop(columns=['Vendor/supplying plant','Outline Agreement', 'Deletion Indicator','Req. Tracking Number'])\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "output_filename = 'output/final.xlsx'\n",
    "\n",
    "df_final.to_excel(output_filename, sheet_name='data', engine='xlsxwriter',index=False)\n",
    "print('done')\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start = time.time()\n",
    "# df1.to_csv('output/final_csv.csv', index=False)\n",
    "# end = time.time()\n",
    "# print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create PO Summary List\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['Net Order Value'] = df1['Net Order Value'].str.replace(',','')\n",
    "df1['Net Order Value']= df1['Net Order Value'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df1.groupby(['Vendor','Purchasing Document','Capex/Opex', 'Currency','Document Date']).agg('sum')\n",
    "df2.drop(columns=['Outline Agreement','Aging Days','PO Year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv('test/po_site.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
